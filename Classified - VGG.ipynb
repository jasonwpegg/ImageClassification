{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:33.026696100Z",
     "start_time": "2024-02-03T02:11:32.964884100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.426176400Z",
     "start_time": "2024-02-03T02:11:33.028649500Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as k, regularizers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set hyperprameters and top-level variables\n",
    "data_dir = 'data3'\n",
    "imgsize = 325\n",
    "batchsize = 16\n",
    "max_epochs = 64\n",
    "wght_decay = 0.00005\n",
    "learning_rate = 0.01\n",
    "learning_decay = 1e-06\n",
    "learning_drop = 20\n",
    "learning_momentum = 0.9\n",
    "epsilon = 1e-08"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.438479400Z",
     "start_time": "2024-02-03T02:11:34.427152800Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Hide the GPU - forces CPU training\n",
    "# tf.config.set_visible_devices([], 'GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.439456400Z",
     "start_time": "2024-02-03T02:11:34.431112400Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.479682900Z",
     "start_time": "2024-02-03T02:11:34.435546100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "   tf.config.experimental.set_memory_growth(gpu, True)\n",
    "   #tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.492910600Z",
     "start_time": "2024-02-03T02:11:34.482609300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.897626900Z",
     "start_time": "2024-02-03T02:11:34.487042600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 950 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory(data_dir, batch_size=batchsize, image_size=(imgsize, imgsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:34.917246700Z",
     "start_time": "2024-02-03T02:11:34.899129900Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda x,y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# take a look at the data\n",
    "data_iterator = data.as_numpy_iterator()\n",
    "batch = data_iterator.next()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.124332300Z",
     "start_time": "2024-02-03T02:11:34.911387200Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(16, 325, 325, 3)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.154443500Z",
     "start_time": "2024-02-03T02:11:35.128774600Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.00784314, 0.00784314, 0.        ],\n        [0.00784314, 0.00784314, 0.        ],\n        [0.00467572, 0.00467572, 0.        ],\n        ...,\n        [0.02342379, 0.02734536, 0.0066453 ],\n        [0.00392157, 0.00447059, 0.        ],\n        [0.00392157, 0.00460068, 0.        ]],\n\n       [[0.00784314, 0.00784314, 0.        ],\n        [0.00784314, 0.00784314, 0.        ],\n        [0.00467572, 0.00467572, 0.        ],\n        ...,\n        [0.01568628, 0.01960784, 0.        ],\n        [0.01341176, 0.01733333, 0.        ],\n        [0.01341176, 0.01733333, 0.        ]],\n\n       [[0.00784314, 0.00784314, 0.        ],\n        [0.00784314, 0.00784314, 0.        ],\n        [0.00689291, 0.00689291, 0.        ],\n        ...,\n        [0.0145098 , 0.01843137, 0.        ],\n        [0.01779737, 0.02171894, 0.        ],\n        [0.01547449, 0.01939606, 0.        ]],\n\n       ...,\n\n       [[0.44951722, 0.4652035 , 0.5083408 ],\n        [0.6908584 , 0.7065447 , 0.74968195],\n        [0.5106637 , 0.52634996, 0.5694872 ],\n        ...,\n        [0.6938014 , 0.5094877 , 0.48595828],\n        [0.68856657, 0.50425285, 0.4807234 ],\n        [0.6604976 , 0.47618392, 0.4526545 ]],\n\n       [[0.45927948, 0.47496575, 0.518103  ],\n        [0.5418595 , 0.5575458 , 0.60068303],\n        [0.5633044 , 0.5789907 , 0.62212795],\n        ...,\n        [0.69754374, 0.51323   , 0.48970062],\n        [0.68918854, 0.50487477, 0.4813454 ],\n        [0.6609224 , 0.47660866, 0.45307925]],\n\n       [[0.52883196, 0.54451823, 0.5876555 ],\n        [0.5770854 , 0.59277165, 0.6359089 ],\n        [0.56340784, 0.5790942 , 0.6222314 ],\n        ...,\n        [0.67613024, 0.4918165 , 0.46828708],\n        [0.66919684, 0.48488313, 0.46135372],\n        [0.65516967, 0.47085595, 0.44732654]]], dtype=float32)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.216239200Z",
     "start_time": "2024-02-03T02:11:35.138278300Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.242250700Z",
     "start_time": "2024-02-03T02:11:35.150537100Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 2000x2000 with 5 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:5]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    # ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.909901Z",
     "start_time": "2024-02-03T02:11:35.161815300Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Split Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.922898200Z",
     "start_time": "2024-02-03T02:11:35.911118700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "60"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.923874200Z",
     "start_time": "2024-02-03T02:11:35.917007700Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)\n",
    "test_size = int(len(data)*.1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  42\n",
      "Val size:  12\n",
      "Test size:  6\n",
      "Total size:  60\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: \", train_size)\n",
    "print(\"Val size: \", val_size)\n",
    "print(\"Test size: \", test_size)\n",
    "print(\"Total size: \", train_size+val_size+test_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.925827400Z",
     "start_time": "2024-02-03T02:11:35.920945100Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.962130900Z",
     "start_time": "2024-02-03T02:11:35.924850500Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for images, labels in train.take(1):  # only take first element of dataset\n",
    "#     numpy_images = images.numpy()\n",
    "#     numpy_labels = labels.numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.963107600Z",
     "start_time": "2024-02-03T02:11:35.931239900Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# numpy_images*255"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.964083900Z",
     "start_time": "2024-02-03T02:11:35.935146300Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# index=32\n",
    "# plt.imshow(numpy_images[index])\n",
    "# plt.xlabel(numpy_labels[index])\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.965060500Z",
     "start_time": "2024-02-03T02:11:35.938076400Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# function to determine rough model memory requirements\n",
    "def get_model_memory_usage(batch_size, model):\n",
    "    shapes_mem_count = 0\n",
    "    internal_model_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        layer_type = l.__class__.__name__\n",
    "        if layer_type == 'Model':\n",
    "            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n",
    "        single_layer_mem = 1\n",
    "        out_shape = l.output_shape\n",
    "        if type(out_shape) is list:\n",
    "            out_shape = out_shape[0]\n",
    "        for s in out_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([k.count_params(p) for p in model.trainable_weights])\n",
    "    non_trainable_count = np.sum([k.count_params(p) for p in model.non_trainable_weights])\n",
    "\n",
    "    number_size = 4.0\n",
    "    if k.floatx() == 'float16':\n",
    "        number_size = 2.0\n",
    "    if k.floatx() == 'float64':\n",
    "        number_size = 8.0\n",
    "\n",
    "    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n",
    "    return gbytes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.966037200Z",
     "start_time": "2024-02-03T02:11:35.942513300Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:35.967013900Z",
     "start_time": "2024-02-03T02:11:35.947398900Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.148471500Z",
     "start_time": "2024-02-03T02:11:35.958670700Z"
    }
   },
   "outputs": [],
   "source": [
    "# initial setup\n",
    "model.add(Conv2D(64, (3,3), 1, padding='same', activation='relu', input_shape=(imgsize,imgsize,3), kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# layers\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(wght_decay)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.162674700Z",
     "start_time": "2024-02-03T02:11:36.149447800Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up the optimizer and compile the model\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=1e-2,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.9)\n",
    "# optimizer = optimizers.sgd_experimental.SGD(learning_rate=lr_schedule)\n",
    "optimizer = optimizers.gradient_descent_v2.SGD(learning_rate=learning_rate, decay=learning_decay, momentum=learning_momentum, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# set up a weights checkpoint callback\n",
    "filepath = os.path.join('models','weights.best.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.163651400Z",
     "start_time": "2024-02-03T02:11:36.158767800Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.276921500Z",
     "start_time": "2024-02-03T02:11:36.161698100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 325, 325, 64)      1792      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 325, 325, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 325, 325, 64)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 325, 325, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 325, 325, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 162, 162, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 162, 162, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 162, 162, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 162, 162, 128)     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 162, 162, 128)     147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 162, 162, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 81, 81, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 81, 81, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 81, 81, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 81, 81, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 81, 81, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 81, 81, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 81, 81, 256)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 81, 81, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 81, 81, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 40, 40, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 40, 40, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 40, 40, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 40, 40, 512)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 40, 40, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 40, 40, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 40, 40, 512)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 40, 40, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 40, 40, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 20, 20, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 20, 20, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 20, 20, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 20, 20, 512)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 20, 20, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 20, 20, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 20, 20, 512)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 20, 20, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 20, 20, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 10, 10, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 10, 10, 512)       0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 51200)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               26214912  \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,949,057\n",
      "Trainable params: 40,939,585\n",
      "Non-trainable params: 9,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "4.654"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit around 6GB???\n",
    "get_model_memory_usage(batchsize, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.292911300Z",
     "start_time": "2024-02-03T02:11:36.198983Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.293914100Z",
     "start_time": "2024-02-03T02:11:36.204843400Z"
    }
   },
   "outputs": [],
   "source": [
    "# logdir='logs'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "# hist = model.fit(train, epochs=epochs, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-03T02:11:36.296846900Z",
     "start_time": "2024-02-03T02:11:36.209282700Z"
    }
   },
   "outputs": [],
   "source": [
    "# trained_model = model.fit_generator(image_augm.flow(x_train, y_train,\n",
    "#     batch_size=batch_size),\n",
    "#     steps_per_epoch=x_train.shape[0]//batch_size,\n",
    "#     epochs=maxepoches,\n",
    "#     validation_data=(x_test, y_test),callbacks=callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3820 - accuracy: 0.6399\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55729, saving model to models\\weights.best.hdf5\n",
      "42/42 [==============================] - 16s 273ms/step - loss: 0.3820 - accuracy: 0.6399 - val_loss: 0.4751 - val_accuracy: 0.5573\n",
      "Epoch 2/64\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6097 - accuracy: 0.6443\n",
      "Epoch 2: val_accuracy did not improve from 0.55729\n",
      "42/42 [==============================] - 9s 212ms/step - loss: 0.6097 - accuracy: 0.6443 - val_loss: 1.1596 - val_accuracy: 0.4062\n",
      "Epoch 3/64\n",
      "42/42 [==============================] - ETA: 0s - loss: 17.7453 - accuracy: 0.4926\n",
      "Epoch 3: val_accuracy did not improve from 0.55729\n",
      "42/42 [==============================] - 9s 212ms/step - loss: 17.7453 - accuracy: 0.4926 - val_loss: 32.5559 - val_accuracy: 0.4167\n",
      "Epoch 4/64\n",
      "42/42 [==============================] - ETA: 0s - loss: 93.3663 - accuracy: 0.4732\n",
      "Epoch 4: val_accuracy improved from 0.55729 to 0.66146, saving model to models\\weights.best.hdf5\n",
      "42/42 [==============================] - 10s 240ms/step - loss: 93.3663 - accuracy: 0.4732 - val_loss: 185.8097 - val_accuracy: 0.6615\n",
      "Epoch 5/64\n",
      "42/42 [==============================] - ETA: 0s - loss: 3052.4414 - accuracy: 0.4643\n",
      "Epoch 5: val_accuracy did not improve from 0.66146\n",
      "42/42 [==============================] - 9s 212ms/step - loss: 3052.4414 - accuracy: 0.4643 - val_loss: 13444.9736 - val_accuracy: 0.5000\n",
      "Epoch 6/64\n",
      "23/42 [===============>..............] - ETA: 3s - loss: 20810.1406 - accuracy: 0.4755"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\engine\\training.py:1389\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1387\u001B[0m logs \u001B[38;5;241m=\u001B[39m tmp_logs  \u001B[38;5;66;03m# No error, now safe to assign to logs.\u001B[39;00m\n\u001B[0;32m   1388\u001B[0m end_step \u001B[38;5;241m=\u001B[39m step \u001B[38;5;241m+\u001B[39m data_handler\u001B[38;5;241m.\u001B[39mstep_increment\n\u001B[1;32m-> 1389\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mend_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n\u001B[0;32m   1391\u001B[0m   \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:438\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    432\u001B[0m \n\u001B[0;32m    433\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    436\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 438\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModeKeys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTRAIN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mend\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:297\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    295\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m hook \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 297\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_end_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    300\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUnrecognized hook: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Expected values are [\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:318\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m   batch_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_start_time\n\u001B[0;32m    316\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times\u001B[38;5;241m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 318\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_batch_hook_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_times) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    321\u001B[0m   end_hook_name \u001B[38;5;241m=\u001B[39m hook_name\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:356\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    355\u001B[0m   hook \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 356\u001B[0m   \u001B[43mhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_timing:\n\u001B[0;32m    359\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m hook_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:1034\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m-> 1034\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_batch_update_progbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\callbacks.py:1106\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1102\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m add_seen\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1105\u001B[0m   \u001B[38;5;66;03m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m-> 1106\u001B[0m   logs \u001B[38;5;241m=\u001B[39m \u001B[43mtf_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msync_to_numpy_or_python_type\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1107\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseen, \u001B[38;5;28mlist\u001B[39m(logs\u001B[38;5;241m.\u001B[39mitems()), finalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\utils\\tf_utils.py:563\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\n\u001B[0;32m    561\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(t) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m--> 563\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_single_numpy_or_python_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    910\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    911\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 914\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [func(\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    915\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    910\u001B[0m flat_structure \u001B[38;5;241m=\u001B[39m (flatten(s, expand_composites) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m structure)\n\u001B[0;32m    911\u001B[0m entries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mflat_structure)\n\u001B[0;32m    913\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 914\u001B[0m     structure[\u001B[38;5;241m0\u001B[39m], [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m entries],\n\u001B[0;32m    915\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\keras\\utils\\tf_utils.py:557\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    555\u001B[0m   \u001B[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    556\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, tf\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 557\u001B[0m     t \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    558\u001B[0m   \u001B[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001B[39;00m\n\u001B[0;32m    559\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (np\u001B[38;5;241m.\u001B[39mndarray, np\u001B[38;5;241m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1201\u001B[0m \n\u001B[0;32m   1202\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1221\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1222\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1223\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32mG:\\Development\\Python\\ImageClassification\\.venv-9\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_numpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1188\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1190\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(train, epochs=max_epochs, validation_data=val, callbacks=callbacks_list, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T02:12:34.881422600Z",
     "start_time": "2024-02-03T02:11:36.213187800Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([None] + trained_model.history['loss'], 'o-')\n",
    "ax.plot([None] + trained_model.history['val_loss'], 'x-')\n",
    "ax.legend(['Training loss', 'Validation loss'], loc = 0)\n",
    "ax.set_title('Training/Validation Loss per Epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "# plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "# plt.ylim(bottom=0)\n",
    "# fig.suptitle('Loss', fontsize=20)\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([None] + trained_model.history['accuracy'], 'o-')\n",
    "ax.plot([None] + trained_model.history['val_accuracy'], 'x-')\n",
    "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
    "ax.set_title('Training/Validation Accuracy per Epoch')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "# plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "# plt.ylim(0, 1)\n",
    "# fig.suptitle('Accuracy', fontsize=20)\n",
    "# plt.legend(loc=\"upper left\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Precision:       \", pre.result())\n",
    "print(\"Recall:          \", re.result())\n",
    "print(\"Binary Accuracy: \", acc.result())\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img0 = cv2.imread('test/898.jpg')\n",
    "img1 = cv2.imread('test/1241.jpg')\n",
    "img2 = cv2.imread('test/1321.jpg')\n",
    "img3 = cv2.imread('test/1367.jpg')\n",
    "img4 = cv2.imread('test/1450.jpg')\n",
    "\n",
    "img5 = cv2.imread('test/1547.jpg')\n",
    "img6 = cv2.imread('test/1430.jpg')\n",
    "img7 = cv2.imread('test/1436.jpg')\n",
    "img8 = cv2.imread('test/1466.jpg')\n",
    "img9 = cv2.imread('test/1515.jpg')\n",
    "\n",
    "imgA = cv2.imread('data2/_FAPT/_t01.jpg')\n",
    "imgB = cv2.imread('data2/_FAPT/_t02.jpg')\n",
    "imgC = cv2.imread('data2/_FAPT/_t03.jpg')\n",
    "imgD = cv2.imread('data2/_FAPT/_t04.jpg')\n",
    "imgE = cv2.imread('data2/_FAPT/_t05.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(img0, cv2.COLOR_BGR2RGB))\n",
    "# plt.show()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resize0 = tf.image.resize(img0, (imgsize,imgsize))\n",
    "resize1 = tf.image.resize(img1, (imgsize,imgsize))\n",
    "resize2 = tf.image.resize(img2, (imgsize,imgsize))\n",
    "resize3 = tf.image.resize(img3, (imgsize,imgsize))\n",
    "resize4 = tf.image.resize(img4, (imgsize,imgsize))\n",
    "\n",
    "resize5 = tf.image.resize(img5, (imgsize,imgsize))\n",
    "resize6 = tf.image.resize(img6, (imgsize,imgsize))\n",
    "resize7 = tf.image.resize(img7, (imgsize,imgsize))\n",
    "resize8 = tf.image.resize(img8, (imgsize,imgsize))\n",
    "resize9 = tf.image.resize(img9, (imgsize,imgsize))\n",
    "\n",
    "resizeA = tf.image.resize(imgA, (imgsize,imgsize))\n",
    "resizeB = tf.image.resize(imgB, (imgsize,imgsize))\n",
    "resizeC = tf.image.resize(imgC, (imgsize,imgsize))\n",
    "resizeD = tf.image.resize(imgD, (imgsize,imgsize))\n",
    "resizeE = tf.image.resize(imgE, (imgsize,imgsize))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.imshow(resize0.numpy().astype(int))\n",
    "# plt.show()\n",
    "# plt.imshow(cv2.cvtColor(resize0, cv2.COLOR_BGR2RGB))\n",
    "# plt.show()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# img0"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tf.shape(resize0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# resize0/255"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tf.shape(numpy_images[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# numpy_images[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat0 = model.predict(np.expand_dims(resize0/255, 0))[0][0]\n",
    "yhat1 = model.predict(np.expand_dims(resize1/255, 0))[0][0]\n",
    "yhat2 = model.predict(np.expand_dims(resize2/255, 0))[0][0]\n",
    "yhat3 = model.predict(np.expand_dims(resize3/255, 0))[0][0]\n",
    "yhat4 = model.predict(np.expand_dims(resize4/255, 0))[0][0]\n",
    "\n",
    "yhat5 = model.predict(np.expand_dims(resize5/255, 0))[0][0]\n",
    "yhat6 = model.predict(np.expand_dims(resize6/255, 0))[0][0]\n",
    "yhat7 = model.predict(np.expand_dims(resize7/255, 0))[0][0]\n",
    "yhat8 = model.predict(np.expand_dims(resize8/255, 0))[0][0]\n",
    "yhat9 = model.predict(np.expand_dims(resize9/255, 0))[0][0]\n",
    "\n",
    "yhatA = model.predict(np.expand_dims(resizeA/255, 0))[0][0]\n",
    "yhatB = model.predict(np.expand_dims(resizeB/255, 0))[0][0]\n",
    "yhatC = model.predict(np.expand_dims(resizeC/255, 0))[0][0]\n",
    "yhatD = model.predict(np.expand_dims(resizeD/255, 0))[0][0]\n",
    "yhatE = model.predict(np.expand_dims(resizeE/255, 0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index=25\n",
    "# plt.imshow(numpy_images[index])\n",
    "# plt.xlabel(numpy_labels[index])\n",
    "# plt.show()\n",
    "# yhatX = model.predict(np.expand_dims(numpy_images[index], 0))[0][0]\n",
    "# print(\"{:.11f}\".format(yhatX), \" - Want \", numpy_labels[index])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model(np.expand_dims(resize0, 0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model(np.expand_dims(numpy_images[index], 0))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model(numpy_images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{:.11f}\".format(yhat0), \" - Want 0.0\")\n",
    "print(\"{:.11f}\".format(yhat1), \" - Want 0.0\")\n",
    "print(\"{:.11f}\".format(yhat2), \" - Want 0.0\")\n",
    "print(\"{:.11f}\".format(yhat3), \" - Want 0.0\")\n",
    "print(\"{:.11f}\".format(yhat4), \" - Want 0.0\")\n",
    "print(\"------------------------\")\n",
    "print(\"{:.11f}\".format(yhat5), \" - Want 1.0\")\n",
    "print(\"{:.11f}\".format(yhat6), \" - Want 1.0\")\n",
    "print(\"{:.11f}\".format(yhat7), \" - Want 1.0\")\n",
    "print(\"{:.11f}\".format(yhat8), \" - Want 1.0\")\n",
    "print(\"{:.11f}\".format(yhat9), \" - Want 1.0\")\n",
    "print(\"------------------------\")\n",
    "print(\"{:.11f}\".format(yhatA), \" - Verify 1.0\")\n",
    "print(\"{:.11f}\".format(yhatB), \" - Verify 1.0\")\n",
    "print(\"{:.11f}\".format(yhatC), \" - Verify 1.0\")\n",
    "print(\"{:.11f}\".format(yhatD), \" - Verify 1.0\")\n",
    "print(\"{:.11f}\".format(yhatE), \" - Verify 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('models','imageclassifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model(os.path.join('models', 'imageclassifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhatnew = new_model.predict(np.expand_dims(resize0/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhatnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 12. Release Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras import backend as kbe\n",
    "# kbe.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
